{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip-Gram with Negative Sampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVRjXNVBJvNTnNwWxzbk77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoungMinJu/lab_study/blob/main/Skip_Gram_with_Negative_Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNuhgrzUEHx8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQMrCWv_EM40"
      },
      "source": [
        " 네거티브 샘플링을 이용한 Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHhF-7_IEPLh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PixfzutYETF-",
        "outputId": "0961cfbb-0ce5-413f-ac40-cfbb8aa5644d"
      },
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "print('총 샘플 수 :',len(documents))\n",
        "\n",
        "#하나의 샘플에 최소 2개의 단어가 있어야하는데 이거 만족 안하는 샘플 제거\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "총 샘플 수 : 11314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmnaRvvJEYwf"
      },
      "source": [
        "#전처리\n",
        "\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "# 특수 문자 제거\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "# 전체 단어에 대한 소문자 변환\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1V6EDPEbof",
        "outputId": "588756eb-d273-4d43-cc50-f63cce02f9b2"
      },
      "source": [
        "news_df.isnull().values.any()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDsXPNj3Ed5f",
        "outputId": "5355d34d-e0a1-4e79-c44c-5e877c1bd925"
      },
      "source": [
        "news_df.dropna(inplace=True)\n",
        "print('총 샘플 수 :',len(news_df))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "총 샘플 수 : 11314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ltq2yWWEEeie",
        "outputId": "db566e63-1b3a-449f-97cf-02fbd696fe02"
      },
      "source": [
        "#불용어 제거   nltk에서 제공하는\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaGhsEj8E03f",
        "outputId": "b8a6d6b7-0974-442e-86a1-ca271f579350"
      },
      "source": [
        "# 단어가 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
        "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
        "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
        "print('총 샘플 수 :',len(tokenized_doc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "총 샘플 수 : 10940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIE_dnonE4Z_"
      },
      "source": [
        "#단어 집합 생성 ++++ 정수 인코딩 진행\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L00Y38bE7EO",
        "outputId": "915fb4c9-0e1d-4027-ba21-61c2fe1ec381"
      },
      "source": [
        "#상위 2개의 샘플을 출력\n",
        "print(encoded[:2])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuHuKjTvE-Df",
        "outputId": "68285f22-a69b-4d95-b447-215de010adcd"
      },
      "source": [
        "#단어 집합의 크기 확인\n",
        "vocab_size = len(word2idx) + 1 \n",
        "print('단어 집합의 크기 :', vocab_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 64277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCdD0lWaFA0-"
      },
      "source": [
        "#네거티브 샘플링을 통한 데이터셋 구성하기\n",
        "\n",
        "#위에서 토큰화+정제+정규화+불용어 제거+ 정수 인코딩까지 일반적인 전처리 과정 거쳤음\n",
        "#이제 네거티브 샘플링 통한 데이터셋 구성할 차례\n",
        "\n",
        "#네거티브 샘플링ㅇㄹ 위해서 케라스에서 제공하는 전처리 도구 skipgrams를 사용\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prapWaQbFeYR",
        "outputId": "56cafe99-e193-44f6-965b-d1cea764d657"
      },
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(5):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          idx2word[pairs[i][0]], pairs[i][0], \n",
        "          idx2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))\n",
        "    \n",
        "\n",
        "\n",
        "#윈도우 크기 내에서 중심단어-주변단어 관계가지면 레이블 1가지게 하고\n",
        "#그렇지 않으면 0을 가지게했음\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(guilt (4989), unfortunate (4295)) -> 1\n",
            "(media (702), anonymity (1815)) -> 0\n",
            "(austria (4866), diverse (6381)) -> 0\n",
            "(media (702), disagree (1495)) -> 1\n",
            "(degree (1530), smtl (60918)) -> 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM3HUzH8FpoR",
        "outputId": "5a45ff46-0061-4a7f-faf2-6a203dcf471d"
      },
      "source": [
        "print('전체 샘플 수 :',len(skip_grams))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 샘플 수 : 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irhpphkUFz6A",
        "outputId": "a83f3fca-6279-4905-dead-ae2a96768e08"
      },
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2220\n",
            "2220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS0sivnXF02P"
      },
      "source": [
        "#위의 작업을 모든 뉴스그룹 샘플에 대해서 수행\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqre0wmVGusr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
