{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "트리의 앙상블.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+arbag1PMJMkYEr1bOayi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoungMinJu/machine_learning/blob/main/5%EC%9E%A5/%ED%8A%B8%EB%A6%AC%EC%9D%98_%EC%95%99%EC%83%81%EB%B8%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONgp9K6zXQV9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cworkDfeXT8y"
      },
      "source": [
        "앞서까진 정형데이터를 다루었습니다. 근데 비정형데이터가 아주 많이 조냊\n",
        "\n",
        "정형데이터를 다루는데 ㄱ자ㅏㅇ 뛰어난 성과를 내는 알고리즘이 앙상블학습! 이 알고리즘은 대부분 결정트리를 기반으로 만들어져있다. \n",
        "\n",
        "비정형데이터에는 7장에서 배울 신경망 알고리즘을 사용한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze9xyB7ZXrWr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyrR3JGYXtI7"
      },
      "source": [
        "앙상블 학습의 대표주자 중 하나인 랜덤포레스트!\n",
        "\n",
        "결정트리를 랜덤하게 만들어 결저읕리의 숲을 만드는 것이다. 각 결정트리의 예측을 사용해 최종 예측을 만든다. \n",
        "\n",
        "랜덤포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데 이 데이터를 만드는 방법이 독특하다. 우리가 입력한 데이터에서 랜덤하게 샘플을 추출하여 훈련데이터를만드는데 이때 한 샘플이 중복되어 추출될수도있다. 이렇게 만들어진 샘플을 부트스트랩 샘플이라 부른다. 일반적으로 부트스트랩 샘플을 훈련세트의 크기와 같게 만든다. 예를 들어 1000개의 가방에서 중복하여 1000개의 샘플을 뽑기 때문에 부트스트랩 샘플은 훈련세트와 크기가 같다.\n",
        "\n",
        "\n",
        "또한 각 녿르르 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음에 이 중에서 최선의 분할을 찾는다. 분류모델인 RandomForestClassifier은 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다. 다만 회귀모델인 RandomForestRegressor은 전체 특성을 사용한다. \n",
        "\n",
        "사이킷런의 랜덤포레스트는 기본적으로 100개의 결정트리를 이런 방식으로 훈련한다. 그 다음 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장높은 확률을 가진 클래스를 예측으로 삼는다. 회귀일 떄는 단순히 각 트리의 예측을 평균한다. \n",
        "\n",
        "\n",
        "랜덤하게 선택한 샘플과 특성을 사용하므로 과대적합을 막아주고 검증세트와 테스트 세트에서 안정적인 성능을 얻을 수 있따. 종종 기본 매개변수 선택만으로도 아주 좋은 결과를 낼수 있음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iyarNjtZOYT"
      },
      "source": [
        "#와인 분류를 해보자\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine=pd.read_csv('https://bit.ly/wine-date')\n",
        "data=wine[['alcohol','sugar','pH']].to_numpy()\n",
        "target=wine['class'].to_numpy()\n",
        "train_input, test_input, train_target, test_target=train_test_split(data, target, test_size=0.2, random_state=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxiakzH-ZmZF",
        "outputId": "fd8f50dd-cbfc-4176-a6c1-ec0cd2e4a979"
      },
      "source": [
        "#교차검증 수행!!!!!!!!!!!!!!\n",
        "#랜덤포레스트분류는 기본적으로 100개의 결정트리르 사용한다.\n",
        "#return_train_score 매개변수를 True로 하면 검증 점수 뿐만아니라 훈련세트에 대한 점수도 같이 반환한다. 훈련세트와 검증세트의 점수를 비교하면서 과대적합을 파악할 수 있음\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf=RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores=cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))\n",
        "\n",
        "#과대적합되었지만 알고리즘 조사하는 것이 목적이므로 매개변수를 더이상 조정하지 않을 것"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QvJ5iSBZy3I",
        "outputId": "0140e3a2-0e93-47e2-a1e0-15aa5dd7f0f1"
      },
      "source": [
        "#랜덤 포레스트는 결정트리의 앙상블이므로 결정트리가 제공하는 중요한 매개변수를 모두 제공한다.\n",
        "#그리고 장점은 특성 중요도를 계산한다!!!!이는 결정트리의 장점이기도함\n",
        "#랜덤포레스트의 특성중요도는 각 결정트리의 특성 중요도를 취합한 것\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUb7IB3ebAO1",
        "outputId": "34d2aafc-f948-4d1a-e0d0-5d649d6e47a5"
      },
      "source": [
        "#랜덤포레스트 분류는 자체적으로 모델을 평가하는 점수를 얻을 수 있다. \n",
        "#부트스트랩 샘플에 포함되지 않고 남는 샘플을 OOB 샘플이라하는데 이걸 사용하여 결정트리 모델을 평가할 수 있음\n",
        "\n",
        "#이거 얻으며녀 oob_scores 매개변수를 True로 설정해야한다. \n",
        "\n",
        "rf=RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8934000384837406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7XIFmRsbXRN"
      },
      "source": [
        "#다음에 알아볼 앙상블 학습은 랜덤포레스트와 아주 비슷한 엑스트라 트리입니당\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQfxzdN8bffe"
      },
      "source": [
        "엑스트라 트리는 랜덤포레스트와 매우 비슷하게 동작한다. 기본적으로 100개의 결정트리를 훈련하는데 랜덤포레스트와 동일하게 결정트리가 제공하는 대부분의 매개변수를 지원한다. 또한 전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용한다.\n",
        "\n",
        "차이점은 부트스트랩 샘플을 사용하지 않는다는 것!!!!!!!!!!!!! 결정ㅌ리를 만들 때 전체 훈련세트를 사용한다. 대신 노드를 분할 할때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다. 엑스트라 트리가 사용하는 결정트리가 DecisionTreeClassifier(splitter='random')이다.\n",
        "\n",
        "이렇게 된다면 성능은 낮아지겠지만 많은 트리를 앙상블하기 때문에 과대적합막고 검증세트의 점수를 높이는 효과가 존재한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLgULKswcBys",
        "outputId": "0143081d-fd7f-401f-b87e-d8147b7623f4"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et=ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores=cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoBu0oGtcSIw",
        "outputId": "3d7ab2e1-8202-4ca6-8bb2-250a45408e80"
      },
      "source": [
        "#특성중요도\n",
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoSYDNEecc88"
      },
      "source": [
        "그레이디언트 부스팅은 깊이가 얕은 결정트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법이다.  사이킷런의 GradientBoostingClassifier은 기본적으로 깊이가 3인 결정트리를 100개 사용한다. 얕은 결정트리 사용하므로 과대적합에 강하고 일반적으로 높은 일반화성능 기대할 수 있따. \n",
        "\n",
        "\n",
        "경사하강법을 사용하여 트리를 앙상블에 추가하는 것이다:) 분류에서는 로지스틱 손실함수를 사용하고 회귀에서는 평균 제곱 오차함수를 사용한다.\n",
        "\n",
        "그레디언트 부스팅은 결정트리를 계속 추가하면서 가장 낮은 곳을 찾아 이동한다. 천처니 조금씩 이동해야해서 깊이가 얕은 트리를 사용하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yplBttOPg5Qz",
        "outputId": "8de587a0-6bf4-4843-968a-d182748c7709"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb=GradientBoostingClassifier(random_state=42)\n",
        "scores=cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlN0Ed9bhSHz",
        "outputId": "8c132509-1cfb-43cd-d046-42974d3e828a"
      },
      "source": [
        "gb=GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "#결정트리의 개수를 늘리고 learning_rate 바꿈 (기본값은 0.1)\n",
        "scores=cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux6rzlMmhi4t",
        "outputId": "961d201d-db6b-4c29-fac5-3c7df47a921c"
      },
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bIF67W-hnp-"
      },
      "source": [
        "#트리 훈련에 사용할 훈련세트의 비율을 정하는 것은subsample이다. 기본값은 1으로 전체 훈련세트를 훈련한다.\n",
        "#1보다 작으면 훈련세트의 일부르 사용하는데 이는 경사하가업 단계마다 일부 샘플을 랜덤하게 선택하여 진행하는 확률적 경사하강법이나 미니배치 경사하강법과 비슷하다!!\n",
        "\n",
        "#일반적으로 그레디이언트 부스팅이 랜덤포ㅔ스트보다 조금 더 높ㅇ느 성능을 얻을 수 있다. 하지만 순서대로 트리를 추가해서 훈련 속도가 느리다.\n",
        "#따라서!!!!!!!n_jobs 매개변수가 없다\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs-gxQ4rh9IN"
      },
      "source": [
        "히스토그램기반 그레이디언트 부스팅은 일반 그레이디언트의 속도와 성능을 개선한 것\n",
        "\n",
        "정형 데이터 다루는 알고리즘 중에 가장 인기가 많습니다.\n",
        "먼저 입력 특성을 256개 구간으로 나눈다. 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다. 히스토그램 기반 그레이디언으 부스팅은 256 구간중에서 하나를 떼어놓고 누락된 값을 위해서 사용한다. 따라서 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다.  \n",
        "\n",
        "HistGradientBoostingClassifier은 기본 매개변수에서 안정적인 성능을 얻을 수 있다. 트리 개수 지정하는데 n_estimator대신 max_iter 을 쓴다(반복횟수 지정)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzMeUI1LjB16",
        "outputId": "b7aca6d6-d994-44d9-f8a3-34c1e8e33260"
      },
      "source": [
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "#이렇게 두개 다 임포트 해야함\n",
        "\n",
        "hgb=HistGradientBoostingClassifier(random_state=42)\n",
        "scores=cross_validate(hgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "#과대적합 잘 억제하면서 그레이디언트보다 조금ㄷ ㅓ 높은 성능 재ㅔ공!"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYl1U4MTjbEy"
      },
      "source": [
        "#특성중요도를 계산하기 위해 permutation_importance()함수를 사용한다!!!!!!!!!!\n",
        "#이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변하는지를 관찰하여 어떤 특성이 중요한지를 게산한다.\n",
        "#훈련세트 뿐만 안라 테스트 세트에도 적용할 수 있고 사이킷런에서 제공하는 추정기 모델에 모두 사용할 수 있따.\n",
        "\n",
        "\n",
        "#먼저 히스토그램 기반 그레이디언트 부스팅 모델을 훈련하고 훈련세트에서 특성 중요도를 게산해보겠다.\n",
        "#n_repeats 매개변수는 랜덤하게 섞을 횟수를 지정한다 기본값은 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfaVWTY4cb5N",
        "outputId": "e4da89ed-1017-4073-863f-11f4f9a0f5ed"
      },
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "result=permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APX5NwRykKwD"
      },
      "source": [
        "#permutation_impotance가 반호나하는 객체는 반복하여 얻은 특성 중요도, 평균, 표준편차를 담고있는다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOBFDy-MkQR9",
        "outputId": "b0c354c5-b7c2-45da-ce95-d10b3bd4cbd3"
      },
      "source": [
        "result=permutation_importance(hgb, test_input, test_target, random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05830769 0.20323077 0.04984615]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoK-0ITHkXha",
        "outputId": "0699cc66-926a-45f4-b9d7-643bb7923688"
      },
      "source": [
        "#테스트 세트에서의 성능을 최종적으로 확인\n",
        "\n",
        "hgb.score(test_input, test_target)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLpV9l9VkkIw"
      },
      "source": [
        "히스토그램 기반 그레이디언트 부스팅의 회귀버전도 존재~~~\n",
        "\n",
        "사이킷런 말고도 히스토그램 기반 그레이디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 있음\n",
        "\n",
        "대표적인 라이브러리는 XGBoost!!! 사이킷런의 cross_validate()랑도 함께 쓸 수 있다. tree_method를 his로 지정하면 히스토그램 그레이디언트 부스팅을 사용할 수 있따."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1JB9yzDlzw5",
        "outputId": "e015a0c9-0e2c-452e-dcf7-96bfd0e1ea19"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb=XGBClassifier(tree_method='hist',random_state=42)\n",
        "scores=cross_validate(xgb, train_input, train_target, return_train_score=True)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8824322471423747 0.8726214185237284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_9FbuHckgBO",
        "outputId": "fa2fbbd8-8814-4cda-a330-b015a8f577c2"
      },
      "source": [
        "#다른거는 마이크로소프트에서 만든 LightGBM\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "lgb=LGBMClassifier(random_state=42)\n",
        "scores=cross_validate(xgb, train_input, train_target, return_train_score=True,n_jobs=-1)\n",
        "print(np.mean(scores['train_score']),np.mean(scores['test_score']))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8824322471423747 0.8726214185237284\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}